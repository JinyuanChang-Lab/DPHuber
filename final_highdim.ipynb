{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "This cell imports the necessary Python libraries for the analysis. It includes the custom `final_util` library, `numpy` and `numpy.random` for numerical calculations, `matplotlib.pyplot` for data visualization, and sets the plot style to 'ggplot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_util import *\n",
    "import numpy as np\n",
    "import numpy.random as rgt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Dimensional Data Simulation\n",
    "In this section, we simulate high-dimensional data based on the model:\n",
    "\n",
    "\n",
    "Y = 2 + X β + ε\n",
    "\n",
    "\n",
    "where:\n",
    "-  X  is a matrix of high-dimensional data with entries drawn from a uniform distribution  U[-2,2] .\n",
    "-  β is a sparse parameter vector, where the first 10 entries are non-zero and either 1 or -1, and the rest of the entries are zeros.\n",
    "-  ε is the error term, which can be drawn from a specified distribution (e.g., N(0,1) or  t_{2.25} ).\n",
    "-  The sparsity of the model is 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodel 1: ε ~ N(0, 1) \n",
    "\n",
    "In this submodel, the error term ε is drawn from a standard normal distribution   N(0, 1). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error vs. Iterations for Submodel 1\n",
    "\n",
    "In this subsection, we compare the errors of the high-dimensional gradient descent model `gd_highdim` and the noisy gradient descent model `noisygd_highdim` over multiple iterations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=2000 # dimension\n",
    "n=20000 # sample size\n",
    "n_1 = 200 # sample size for initial estimation\n",
    "T_1 =   4  ## number of iterations for initial estimation\n",
    "n_2 = n - n_1 # the rest sample size for DP Huber \n",
    "s_star = 10 # ture sparsity level\n",
    "s_huber =  2*s_star   \n",
    "lr_huber = 0.1  #  learning rate for noise huber\n",
    "mu,delta = 0.5,0.01 # privacy levels\n",
    "repetitions = 300  # number of repetitions\n",
    "\n",
    "rgt.seed(0) # set seed\n",
    "beta = np.zeros(p)  # initial beta \n",
    "beta[:s_star] =  np.ones(s_star)*(2*rgt.binomial(1, 0.5, size=s_star)-1) # beta:{1,-1,1,...,}\n",
    "beta_true = np.insert(beta, 0, 2) # Adding parameter for the intercept term \n",
    "beta_norm = beta_true.dot(beta_true)**0.5 # the l2 norm of true beta\n",
    "\n",
    "lr_noiseless = 0.5 # learning rate for noiseless huber\n",
    "robust_noiseless = (n_1/(s_huber*np.log(p)+np.log(n_1)))**0.5  # robustification parameter for  initial estimation\n",
    "\n",
    "B_huber = .4*(np.log(p) + np.log(n_2))**0.5  #  ## truncation parameter  \n",
    "T =   int(np.ceil(np.log(n_2)))  ## number of iterations\n",
    "c_0 = .2 # common constant for DP robust parameter\n",
    "robust = c_0 * (n_2/(s_huber*np.log(p)+np.log(n_2)))**0.5 # robustification parameter for  noiseless huber\n",
    "robust_noise = c_0 * (n_2*mu/(s_huber*np.log(p)+np.log(n_2)))**0.5 #   robustification parameter for  noise  huber\n",
    "robust_low1 = .5*(n_2/(s_huber+1+np.log(n_2)))**0.5 # robustification parameter for  noiseless huber: intercept\n",
    "robust_low2 = .5*(n_2*mu/(s_huber+1+np.log(n_2)))**0.5 #   robustification parameter for  noise  huber: intercept\n",
    "\n",
    "Iteration_re_G = np.zeros([T+1, repetitions]) \n",
    "Iteration_re_G_noise = np.zeros([T+1, repetitions])  \n",
    "for m in range(repetitions):\n",
    "    rgt.seed(m+1) \n",
    "    X = np.random.uniform(-2 ,2, (n, p)) # generate X\n",
    "    Y = 2+ X.dot(beta) + np.random.normal(0, 1, n)  # generate Y\n",
    "\n",
    "    random_rows = np.random.choice(n,size=n_1,replace=False)# data split\n",
    "\n",
    "    ## subsample for initial estimation: noiseless huber\n",
    "    X_subsample = X[random_rows]\n",
    "    Y_subsample = Y[random_rows]\n",
    "    model_sub = Huber(X_subsample, Y_subsample,intercept=True)\n",
    "    initial = model_sub.gd_highdim( lr=lr_noiseless, T=T_1,   s=s_huber,tau=None, robust=robust_noiseless, beta0=np.array([]),  standardize= False) \n",
    "\n",
    "    ## rest for DP Huber\n",
    "    X_rest = X[~np.isin(np.arange(n), random_rows)]\n",
    "    Y_rest = Y[~np.isin(np.arange(n), random_rows)] \n",
    "    beta0 = initial['beta']\n",
    "    model = Huber(X_rest, Y_rest,intercept=True)  \n",
    "    out_Huber_noise_new = model.noisygd_highdim( mu=mu , T=T, delta=delta, lr=lr_huber, beta0 =beta0 ,  s=s_huber,robust_low1=robust_low1,robust_low2=robust_low2,robust_high1=robust ,robust_high2=robust_noise, B_high=B_huber,  standardize=False) \n",
    "\n",
    "    # results \n",
    "    Iteration_re_G[:,m] = np.sum((out_Huber_noise_new['beta_seq1']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm \n",
    "    Iteration_re_G_noise[:,m] = np.sum((out_Huber_noise_new['beta_seq2']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors vs. Sample Sizes for Submodel 1\n",
    "\n",
    "In this subsection, we compare the errors of the high-dimensional gradient descent model `gd_highdim`, the noisy gradient descent model `noisygd_highdim_comp`, and the sparse Differential Privacy (DP)  Least Squares `noisygd_ls` model as a function of the sample size. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors vs. Sample Sizes for sparse DP Least Squares (sparse DP LS)  \n",
    "\n",
    "In this subsection, we present the results of applying sparse Differential Privacy (DP) Least Squares to high-dimensional data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "5000\n",
      "10000\n",
      "2500\n",
      "5000\n",
      "10000\n",
      "2500\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "p = 2000\n",
    "s_star = 10 # true sparsity\n",
    "s_ls = 2*s_star # sparsity level \n",
    "repetitions = 300 \n",
    "mu,delta = 0.5,0.01 #  privacy levels  \n",
    "sample_sizes = np.array([2500,5000,10000]) # sample sizes\n",
    "R_coef_cand = np.array([0.1,0.5,1  ]) # candidate coefs for R_ls\n",
    " \n",
    "rgt.seed(0)\n",
    "beta = np.zeros(p) \n",
    "beta[:s_star] = np.ones(s_star)*(2*rgt.binomial(1, 0.5, size=s_star)-1)  \n",
    "beta_norm = beta.dot(beta)**0.5\n",
    "\n",
    "HD_ls_noise_G_all = []\n",
    "for R_coef in R_coef_cand: \n",
    "    HD_ls_noise_G = []\n",
    "    for n in sample_sizes:\n",
    "        print(n)\n",
    "        T = int(np.ceil(np.log(n)))  ## number of iterations  \n",
    "        \n",
    "        lr_ls = 0.1  ## learning rate for DP LS  \n",
    "        C_ls = 1.01 * beta_norm # feasibility parameter for DP LS \n",
    "        R_ls = R_coef * 2 * np.sqrt(2*np.log(n)) # truncation level for DP LS\n",
    "        c_x = 2\n",
    "        B_ls = 4*(R_ls+C_ls*c_x)*c_x/np.sqrt(s_ls) # noise scale for DP LS\n",
    " \n",
    "        HD_ls_noise_G_sub = [] \n",
    "        for m in range(repetitions):\n",
    "            rgt.seed(m+1)\n",
    "            X = np.random.uniform(-2 ,2, (n, p))\n",
    "            Y = 2+X.dot(beta) + np.random.normal(0, 1, n)   \n",
    "\n",
    "            X_cent = X - np.mean(X, axis=0)# Centralization\n",
    "            Y_cent = Y - np.mean(Y)# Centralization\n",
    "            model_ls = Huber(X_cent, Y_cent,intercept=False)\n",
    "            out_LS = model_ls.noisygd_ls(mu=mu , T=T, delta=delta, lr=lr_ls, s=s_ls, R=R_ls,C=C_ls, B=B_ls,beta0=np.array([]), standardize=False)\n",
    "\n",
    "            HD_ls_noise_G_sub.append(np.sum((out_LS['beta'] - beta)**2)**0.5  / beta_norm)  \n",
    "        HD_ls_noise_G.append(np.mean(HD_ls_noise_G_sub))\n",
    "    HD_ls_noise_G_all.append(HD_ls_noise_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors vs. Sample Sizes for High-Dimensional sparse Huber Regression\n",
    "\n",
    "In this subsection, we compare the errors of high-dimensional Huber regression (noiseless and noisy) as a function of sample size. The intercept term is iterated without noise to ensure a fair comparison with the sparse Differential Privacy (DP) Least Squares (LS) method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "p = 2000\n",
    "s_star = 10 \n",
    "s_huber = 2*s_star\n",
    "n_1 = 200\n",
    "T_1 = 4\n",
    "mu,delta = 0.5,0.01 \n",
    "repetitions = 300 \n",
    "sample_sizes = np.array([2500,5000,10000 ])\n",
    "\n",
    "rgt.seed(0)\n",
    "beta = np.zeros(p) \n",
    "beta[:s_star] = np.ones(s_star)*(2*rgt.binomial(1, 0.5, size=s_star)-1)  \n",
    "beta_norm = beta.dot(beta)**0.5\n",
    "\n",
    "robust_noiseless = (n_1/(s_huber*np.log(p)+np.log(n_1)))**0.5\n",
    "lr_noiseless = 0.5 \n",
    "\n",
    "HD_Huber_G = []\n",
    "HD_Huber_noise_G = []\n",
    "for n in sample_sizes:\n",
    "    print(n)\n",
    "    n_2 = n - n_1 \n",
    "\n",
    "    c_0 = .2 # common constant for DP robust parameter\n",
    "    robust = c_0 * (n_2/(s_huber*np.log(p)+np.log(n_2)))**0.5 # robustification parameter for  noiseless huber\n",
    "    robust_noise = c_0 * (n_2*mu/(s_huber*np.log(p)+np.log(n_2)))**0.5    #   robustification parameter for  noise  huber\n",
    "    robust_low1 = .5*(n_2 /(s_huber+1+np.log(n_2)))**0.5 # robustification parameter for  noiseless huber: intercept \n",
    "    \n",
    "    T = int(np.ceil(np.log(n_2)))  ## number of iterations\n",
    "    lr_huber = 0.1  ## learning rate for DP Huber\n",
    "    B_huber =  .4*(np.log(p) + np.log(n_2))**0.5  ## truncation parameter for DP Huber  \n",
    "   \n",
    "    HD_Huber_G_sub = []\n",
    "    HD_Huber_noise_G_sub = []\n",
    "    for m in range(repetitions):\n",
    "        rgt.seed(m+1)\n",
    "        X = np.random.uniform(-2 ,2, (n, p))\n",
    "        Y = 2+X.dot(beta) + np.random.normal(0, 1, n) \n",
    "\n",
    "        ## DP Huber\n",
    "        random_rows = np.random.choice(n,size=n_1,replace=False)\n",
    "        X_subsample = X[random_rows]\n",
    "        Y_subsample = Y[random_rows]\n",
    "        X_rest = X[~np.isin(np.arange(n), random_rows)]\n",
    "        Y_rest = Y[~np.isin(np.arange(n), random_rows)]\n",
    "        model_sub = Huber(X_subsample, Y_subsample,intercept=True)\n",
    "        initial = model_sub.gd_highdim( lr=lr_noiseless, T=T_1,   s=s_huber,tau=None, robust=robust_noiseless, beta0=np.array([]),   standardize= False)  \n",
    "        beta0 = initial['beta']\n",
    "        model_huber = Huber(X_rest, Y_rest,intercept=True)  \n",
    "        out_Huber_noise_new = model_huber.noisygd_highdim_comp( mu=mu , T=T, delta=delta, lr=lr_huber, beta0 =beta0 ,  s=s_huber,robust_low =robust_low1 ,robust_high1=robust ,robust_high2=robust_noise, B_high=B_huber,  standardize=False)\n",
    "\n",
    "\n",
    "    \n",
    "        HD_Huber_G_sub.append(np.sum((out_Huber_noise_new['beta1'][1:]  - beta)**2 )**0.5  / beta_norm )\n",
    "        HD_Huber_noise_G_sub.append(np.sum((out_Huber_noise_new['beta2'][1:]  - beta)**2, axis=0)**0.5  / beta_norm) \n",
    "    HD_Huber_G.append(np.mean(HD_Huber_G_sub))\n",
    "    HD_Huber_noise_G.append(np.mean(HD_Huber_noise_G_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodel 2: ε ~ t_{2.25}\n",
    "\n",
    "In this submodel, the error term ε is drawn from t_{2.25}. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error vs. Iterations for Submodel 2\n",
    "\n",
    "In this subsection, we compare the errors of the high-dimensional gradient descent model `gd_highdim` and the noisy gradient descent model `noisygd_highdim` over multiple iterations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=2000 # dimension\n",
    "n=20000 # sample size\n",
    "n_1 = 200 # sample size for initial estimation\n",
    "T_1 =   4  ## number of iterations for initial estimation\n",
    "n_2 = n - n_1 # the rest sample size for DP Huber \n",
    "s_star = 10 # ture sparsity level\n",
    "s_huber =  2*s_star # \n",
    "lr_huber = 0.1  #  learning rate for noise huber\n",
    "mu,delta = 0.5,0.01 # privacy levels\n",
    "repetitions = 300  # number of repetitions\n",
    "\n",
    "rgt.seed(0) # set seed\n",
    "beta = np.zeros(p)  # initial beta \n",
    "beta[:s_star] =  np.ones(s_star)*(2*rgt.binomial(1, 0.5, size=s_star)-1) # beta:{1,-1,1,...,}\n",
    "beta_true = np.insert(beta, 0, 2) # Adding parameter for the intercept term \n",
    "beta_norm = beta_true.dot(beta_true)**0.5 # the l2 norm of true beta\n",
    "\n",
    "lr_noiseless = 0.5 # learning rate for noiseless huber \n",
    "robust_noiseless = (n_1/(s_huber*np.log(p)+np.log(n_1)))**0.5  # robustification parameter for  initial estimation\n",
    "\n",
    "B_huber = .4*(np.log(p) + np.log(n_2))**0.5  #  ## truncation parameter  \n",
    "T =   int(np.ceil(np.log(n_2)))  ## number of iterations\n",
    "c_0 = .2 # common constant for DP robust parameter\n",
    "robust = c_0 * (n_2/(s_huber*np.log(p)+np.log(n_2)))**0.5 # robustification parameter for  noiseless huber\n",
    "robust_noise = c_0 * (n_2*mu/(s_huber*np.log(p)+np.log(n_2)))**0.5 #   robustification parameter for  noise  huber\n",
    "robust_low1 = .5*(n_2/(s_huber+1+np.log(n_2)))**0.5 # robustification parameter for  noiseless huber: intercept\n",
    "robust_low2 = .5*(n_2*mu/(s_huber+1+np.log(n_2)))**0.5 #   robustification parameter for  noise  huber: intercept\n",
    "\n",
    "Iteration_re_t = np.zeros([T+1, repetitions]) \n",
    "Iteration_re_t_noise = np.zeros([T+1, repetitions])  \n",
    "for m in range(repetitions):\n",
    "    rgt.seed(m+1) \n",
    "    X = np.random.uniform(-2 ,2, (n, p)) # generate X\n",
    "    Y = 2+ X.dot(beta) + rgt.standard_t(2.25, n) # generate Y\n",
    "\n",
    "    random_rows = np.random.choice(n,size=n_1,replace=False)# data split\n",
    "\n",
    "    ## subsample for initial estimation: noiseless huber\n",
    "    X_subsample = X[random_rows]\n",
    "    Y_subsample = Y[random_rows]\n",
    "    model_sub = Huber(X_subsample, Y_subsample,intercept=True)\n",
    "    initial = model_sub.gd_highdim( lr=lr_noiseless, T=T_1,   s=s_huber,tau=None, robust=robust_noiseless, beta0=np.array([]),  standardize= False) \n",
    "\n",
    "    ## rest for DP Huber\n",
    "    X_rest = X[~np.isin(np.arange(n), random_rows)]\n",
    "    Y_rest = Y[~np.isin(np.arange(n), random_rows)] \n",
    "    beta0 = initial['beta']\n",
    "    model = Huber(X_rest, Y_rest,intercept=True)  \n",
    "    out_Huber_noise_new = model.noisygd_highdim( mu=mu , T=T, delta=delta, lr=lr_huber, beta0 =beta0 ,  s=s_huber,robust_low1=robust_low1,robust_low2=robust_low2,robust_high1=robust ,robust_high2=robust_noise, B_high=B_huber,  standardize=False) \n",
    "\n",
    "    # results \n",
    "    Iteration_re_t[:,m] = np.sum((out_Huber_noise_new['beta_seq1']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm \n",
    "    Iteration_re_t_noise[:,m] = np.sum((out_Huber_noise_new['beta_seq2']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors vs. Sample Sizes for Submodel 2\n",
    "\n",
    "In this subsection, we compare the errors of the high-dimensional gradient descent model `gd_highdim`, the noisy gradient descent model `noisygd_highdim_comp`, and the sparse Differential Privacy (DP) Least Squares `noisygd_ls` model as a function of the sample size. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors vs. Sample Sizes for sparse DP Least Squares (sparse DP LS)  \n",
    "\n",
    "In this subsection, we present the results of applying sparse Differential Privacy (DP) Least Squares to high-dimensional data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "5000\n",
      "10000\n",
      "2500\n",
      "5000\n",
      "10000\n",
      "2500\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "p = 2000\n",
    "s_star = 10 # true sparsity\n",
    "s_ls = 2*s_star # sparsity level \n",
    "repetitions = 300 \n",
    "mu,delta = 0.5,0.01 #  privacy levels  \n",
    "sample_sizes = np.array([2500,5000,10000]) # sample sizes\n",
    "R_coef_cand = np.array([0.1,0.5,1  ]) # candidate coefs for R_ls\n",
    " \n",
    "rgt.seed(0)\n",
    "beta = np.zeros(p) \n",
    "beta[:s_star] = np.ones(s_star)*(2*rgt.binomial(1, 0.5, size=s_star)-1)  \n",
    "beta_norm = beta.dot(beta)**0.5\n",
    "\n",
    "HD_ls_noise_t_all = []\n",
    "for R_coef in R_coef_cand: \n",
    "    HD_ls_noise_t = []\n",
    "    for n in sample_sizes:\n",
    "        print(n)\n",
    "        T = int(np.ceil(np.log(n)))  ## number of iterations  \n",
    "        \n",
    "        lr_ls = 0.1  ## learning rate for DP LS  \n",
    "        C_ls = 1.01 * beta_norm # feasibility parameter for DP LS \n",
    "        R_ls = R_coef * 2 * np.sqrt(2*np.log(n)) # truncation level for DP LS\n",
    "        c_x = 2\n",
    "        B_ls = 4*(R_ls+C_ls*c_x)*c_x/np.sqrt(s_ls) # noise scale for DP LS\n",
    " \n",
    "        HD_ls_noise_t_sub = [] \n",
    "        for m in range(repetitions):\n",
    "            rgt.seed(m+1)\n",
    "            X = np.random.uniform(-2 ,2, (n, p))\n",
    "            Y = 2+X.dot(beta) + rgt.standard_t(2.25, n)   \n",
    "\n",
    "            X_cent = X - np.mean(X, axis=0)# Centralization\n",
    "            Y_cent = Y - np.mean(Y)# Centralization\n",
    "            model_ls = Huber(X_cent, Y_cent,intercept=False)\n",
    "            out_LS = model_ls.noisygd_ls(mu=mu , T=T, delta=delta, lr=lr_ls, s=s_ls, R=R_ls,C=C_ls, B=B_ls,beta0=np.array([]), standardize=False)\n",
    "\n",
    "            HD_ls_noise_t_sub.append(np.sum((out_LS['beta'] - beta)**2)**0.5  / beta_norm)  \n",
    "        HD_ls_noise_t.append(np.mean(HD_ls_noise_t_sub))\n",
    "    HD_ls_noise_t_all.append(HD_ls_noise_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors vs. Sample Sizes for High-Dimensional Huber Regression\n",
    "\n",
    "In this subsection, we compare the errors of high-dimensional Huber regression (noiseless and noisy) as a function of sample size. The intercept term is iterated without noise to ensure a fair comparison with the sparse Differential Privacy (DP) Least Squares (LS) method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "p = 2000\n",
    "s_star = 10 \n",
    "s_huber = 2*s_star\n",
    "n_1 = 200\n",
    "T_1 = 4\n",
    "mu,delta = 0.5,0.01 \n",
    "repetitions = 300 \n",
    "sample_sizes = np.array([2500,5000,10000 ])\n",
    "\n",
    "rgt.seed(0)\n",
    "beta = np.zeros(p) \n",
    "beta[:s_star] = np.ones(s_star)*(2*rgt.binomial(1, 0.5, size=s_star)-1)  \n",
    "beta_norm = beta.dot(beta)**0.5\n",
    "\n",
    "robust_noiseless = (n_1/(s_huber*np.log(p)+np.log(n_1)))**0.5\n",
    "lr_noiseless = 0.5 \n",
    "\n",
    "HD_Huber_t = []\n",
    "HD_Huber_noise_t = []\n",
    "for n in sample_sizes:\n",
    "    print(n)\n",
    "    n_2 = n - n_1 \n",
    "\n",
    "    c_0 = .2 # common constant for DP robust parameter\n",
    "    robust = c_0 * (n_2/(s_huber*np.log(p)+np.log(n_2)))**0.5 # robustification parameter for  noiseless huber\n",
    "    robust_noise = c_0 * (n_2*mu/(s_huber*np.log(p)+np.log(n_2)))**0.5    #   robustification parameter for  noise  huber\n",
    "    robust_low1 = .5*(n_2 /(s_huber+1+np.log(n_2)))**0.5 # robustification parameter for  noiseless huber: intercept \n",
    "    \n",
    "    T = int(np.ceil(np.log(n_2)))  ## number of iterations\n",
    "    lr_huber = 0.1  ## learning rate for DP Huber\n",
    "    B_huber =  .4*(np.log(p) + np.log(n_2))**0.5  ## truncation parameter for DP Huber  \n",
    "   \n",
    "    HD_Huber_t_sub = []\n",
    "    HD_Huber_noise_t_sub = []\n",
    "    for m in range(repetitions):\n",
    "        rgt.seed(m+1)\n",
    "        X = np.random.uniform(-2 ,2, (n, p))\n",
    "        Y = 2+X.dot(beta) + rgt.standard_t(2.25, n)  \n",
    "\n",
    "        ## DP Huber\n",
    "        random_rows = np.random.choice(n,size=n_1,replace=False)\n",
    "        X_subsample = X[random_rows]\n",
    "        Y_subsample = Y[random_rows]\n",
    "        X_rest = X[~np.isin(np.arange(n), random_rows)]\n",
    "        Y_rest = Y[~np.isin(np.arange(n), random_rows)]\n",
    "        model_sub = Huber(X_subsample, Y_subsample,intercept=True)\n",
    "        initial = model_sub.gd_highdim( lr=lr_noiseless, T=T_1,   s=s_huber,tau=None, robust=robust_noiseless, beta0=np.array([]),  standardize= False)  \n",
    "        beta0 = initial['beta']\n",
    "        model_huber = Huber(X_rest, Y_rest,intercept=True)  \n",
    "        out_Huber_noise_new = model_huber.noisygd_highdim_comp( mu=mu , T=T, delta=delta, lr=lr_huber, beta0 =beta0 ,  s=s_huber,robust_low =robust_low1 ,robust_high1=robust ,robust_high2=robust_noise, B_high=B_huber,  standardize=False)\n",
    "\n",
    "\n",
    "    \n",
    "        HD_Huber_t_sub.append(np.sum((out_Huber_noise_new['beta1'][1:]  - beta)**2 )**0.5  / beta_norm )\n",
    "        HD_Huber_noise_t_sub.append(np.sum((out_Huber_noise_new['beta2'][1:]  - beta)**2, axis=0)**0.5  / beta_norm) \n",
    "    HD_Huber_t.append(np.mean(HD_Huber_t_sub))\n",
    "    HD_Huber_noise_t.append(np.mean(HD_Huber_noise_t_sub))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
