{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "This cell imports the necessary Python libraries for the analysis. It includes the custom `final_util` library, `numpy` and `numpy.random` for numerical calculations, `matplotlib.pyplot` for data visualization, and sets the plot style to 'ggplot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_util import *\n",
    "import numpy as np\n",
    "import numpy.random as rgt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Dimensional Data Simulation\n",
    "In this section, we simulate low-dimensional data based on the model:\n",
    "\n",
    "\n",
    "Y = 1 + X β + ε\n",
    "\n",
    "\n",
    "where the parameter vector β is defined as \\{1, -1, 1, -1, ...\\}, with each value being either 1 or -1.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodel 1: Data Generation\n",
    "In this submodel, we generate the data as follows:\n",
    "\n",
    "- The predictor matrix  X  follows a normal distribution:  X ~ N(0,1) .\n",
    "- The error term  ε  follows a t-distribution with 2.25 degrees of freedom:  ε ~ t_{2.25}.\n",
    "\n",
    "This model is used to simulate data based on the assumption of normally distributed predictors and error terms with heavy tails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Errors for Submodel 1: gd vs. noisygd\n",
    "In this subsection, we compare the errors of the following models as a function of the number of iterations:\n",
    "\n",
    "- **gd**: The non-noisy gradient descent model.\n",
    "- **noisygd**: The gradient descent model with different privacy parameters.\n",
    "\n",
    "We will examine how the error evolves over iterations for three different privacy parameter settings in the `noisygd` model. The goal is to assess the impact of privacy on the model’s performance and understand how the error changes with respect to iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 4000, 10 # sample size and dimension\n",
    "t_df = 2.25 # degree of freedom for t distribution\n",
    "lr = .5 # learning rate\n",
    "c0 = .5 # common coefficient\n",
    "mu = np.array([.3, .5 ,.9]) # privacy levels for GDP\n",
    "repetitions = 300 # number of repetitions\n",
    "\n",
    "rgt.seed(0) # set seed\n",
    "beta = np.ones(p)*(2*rgt.binomial(1, 0.5, size=p)-1) #   beta: {1,-1,1,...} \n",
    "beta_true = np.insert(beta, 0, 1) # Adding parameter for the intercept term \n",
    "beta_norm = beta_true.dot(beta_true)**0.5 # the l2 norm of true beta\n",
    " \n",
    "\n",
    "T = int(np.ceil(np.log(n))) # number of iterations\n",
    "B = (p + np.log(n))**0.5 # truncation parameter for noise Gaussian DP: Gaussian design\n",
    "robust = c0 * (n/(p+np.log(n)))**0.5 # robustification parameter for noiseless Gaussian DP \n",
    "\n",
    "\n",
    "mse_G_t = np.zeros([T+1, repetitions])\n",
    "priv_mse1_G_t = np.zeros([T+1, repetitions])\n",
    "priv_mse2_G_t = np.zeros([T+1, repetitions])\n",
    "priv_mse3_G_t = np.zeros([T+1, repetitions])\n",
    "for m in range(repetitions):\n",
    "    rgt.seed(m+1)\n",
    "    X = rgt.normal(0, 1, size=(n,p)) # generate X \n",
    "    Y = 1 + X.dot(beta) + rgt.standard_t(t_df, n) # generate Y\n",
    "    \n",
    "    model = Huber(X, Y)\n",
    "    out0 = model.gd(robust, lr=lr, max_niter=T)\n",
    "\n",
    "    priv_robust = c0 * mad(out0['residuals']) * (n*mu/(p+np.log(n)))**0.5 # robustification parameter for noise  Gaussian DP \n",
    "    out1 = model.noisygd(priv_robust[0], lr=lr, B=B, mu=mu[0], T=T)\n",
    "    out2 = model.noisygd(priv_robust[1], lr=lr, B=B, mu=mu[1], T=T)\n",
    "    out3 = model.noisygd(priv_robust[2], lr=lr, B=B, mu=mu[2], T=T)  \n",
    "    \n",
    "    mse_G_t[:,m] = np.sum((out0['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse1_G_t[:,m] = np.sum((out1['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse2_G_t[:,m] = np.sum((out2['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse3_G_t[:,m] = np.sum((out3['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Errors for Submodel 1: gd vs. noisygd with Varying Sample Sizes\n",
    "In this subsection, we compare the errors of the following models as a function of the sample size:\n",
    "\n",
    "- **gd**: The non-noisy gradient descent model.\n",
    "- **noisygd**: The gradient descent model with different privacy parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 10 # dimension \n",
    "t_df = 2.25 # degree of freedom for t distribution\n",
    "lr = .5 # learning rate\n",
    "c0 = .5 # common coefficient\n",
    "mu = np.array([.3, .5 ,.9]) # privacy levels for GDP\n",
    "repetitions = 300 # number of repetitions\n",
    "sample_sizes = np.array(range(2000, 30001, 2000)) # range of sample sizes\n",
    "\n",
    "rgt.seed(0) # set seed\n",
    "beta = np.ones(p)*(2*rgt.binomial(1, 0.5, size=p)-1) #   beta: {1,-1,1,...} \n",
    "beta_true = np.insert(beta, 0, 1) # Adding parameter for the intercept term \n",
    "beta_norm = beta_true.dot(beta_true)**0.5  # the l2 norm of true beta\n",
    "\n",
    "mean_mse_G_t = []\n",
    "mean_priv_mse1_G_t = []\n",
    "mean_priv_mse2_G_t = []\n",
    "mean_priv_mse3_G_t = []\n",
    "for n in sample_sizes:\n",
    "    errors_G_t = []\n",
    "    errors_priv1_G_t = []\n",
    "    errors_priv2_G_t = []\n",
    "    errors_priv3_G_t = []\n",
    "    T = int(np.ceil(np.log(n))) # number of iterations\n",
    "    B = (p + np.log(n))**0.5 # truncation parameter for noise Gaussian DP: Gaussian design\n",
    "    robust = c0 * (n/(p+np.log(n)))**0.5   # robustification parameter for noiseless Gaussian DP \n",
    "    for m in range(repetitions):\n",
    "        rgt.seed(m+1)\n",
    "        X = rgt.normal(0, 1, size=(n,p))\n",
    "        Y = 1 + X.dot(beta) + rgt.standard_t(t_df, n)\n",
    "\n",
    "        model = Huber(X, Y)\n",
    "        out0 = model.gd(robust, lr=lr, max_niter=T)\n",
    "\n",
    "        priv_robust = c0 * mad(out0['residuals']) * (n*mu/(p+np.log(n)))**0.5 # robustification parameter for noise  Gaussian DP \n",
    "        out1 = model.noisygd(priv_robust[0], lr=lr, B=B, mu=mu[0], T=T)\n",
    "        out2 = model.noisygd(priv_robust[1], lr=lr, B=B, mu=mu[1], T=T)\n",
    "        out3 = model.noisygd(priv_robust[2], lr=lr, B=B, mu=mu[2], T=T)\n",
    "\n",
    "        errors_G_t.append(np.sum((out0['beta'] - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv1_G_t.append(np.sum((out1['beta'] - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv2_G_t.append(np.sum((out2['beta'] - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv3_G_t.append(np.sum((out3['beta'] - beta_true)**2)**0.5  / beta_norm)\n",
    "    mean_mse_G_t.append(np.mean(errors_G_t))\n",
    "    mean_priv_mse1_G_t.append(np.mean(errors_priv1_G_t))\n",
    "    mean_priv_mse2_G_t.append(np.mean(errors_priv2_G_t))\n",
    "    mean_priv_mse3_G_t.append(np.mean(errors_priv3_G_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodel 2: Data Generation\n",
    "In this submodel, we generate the data as follows:\n",
    "\n",
    "- The predictor matrix  X  follows a normal distribution:  X ~ N(0,1) .\n",
    "- The error term  ε  follows a normal distribution:  ε ~ N(0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Errors for Submodel 2: gd vs. noisygd\n",
    "In this subsection, we compare the errors of the following models as a function of the number of iterations:\n",
    "\n",
    "- **gd**: The non-noisy gradient descent model.\n",
    "- **noisygd**: The gradient descent model with different privacy parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 4000, 10  \n",
    "lr = .5\n",
    "c_0 = .5\n",
    "mu = np.array([.3, .5 ,.9])\n",
    "repetitions = 300\n",
    "\n",
    "rgt.seed(0)\n",
    "beta = np.ones(p)*(2*rgt.binomial(1, 0.5, size=p)-1)\n",
    "beta_true = np.insert(beta, 0, 1)\n",
    "beta_norm = beta_true.dot(beta_true)**0.5  \n",
    " \n",
    "T = int(np.ceil(np.log(n)))\n",
    "B = (p + np.log(n))**0.5 \n",
    "robust = c0 * (n/(p+np.log(n)))**0.5 \n",
    "\n",
    "mse_G_G = np.zeros([T+1, repetitions])\n",
    "priv_mse1_G_G = np.zeros([T+1, repetitions])\n",
    "priv_mse2_G_G = np.zeros([T+1, repetitions])\n",
    "priv_mse3_G_G = np.zeros([T+1, repetitions])\n",
    "for m in range(repetitions):\n",
    "    rgt.seed(m+1)\n",
    "    X = rgt.normal(0, 1, size=(n,p))\n",
    "    Y = 1 + X.dot(beta) + rgt.normal(0,1, n)\n",
    "    \n",
    "    model = Huber(X, Y)\n",
    "    out0 = model.gd(robust, lr=lr, max_niter=T)\n",
    "\n",
    "    priv_robust = c0 * mad(out0['residuals']) * (n*mu/(p+np.log(n)))**0.5\n",
    "    out1 = model.noisygd(priv_robust[0], lr=lr, B=B, mu=mu[0], T=T)\n",
    "    out2 = model.noisygd(priv_robust[1], lr=lr, B=B, mu=mu[1], T=T)\n",
    "    out3 = model.noisygd(priv_robust[2], lr=lr, B=B, mu=mu[2], T=T)\n",
    "    \n",
    "    mse_G_G[:,m] = np.sum((out0['beta_seq'] - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse1_G_G[:,m] = np.sum((out1['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse2_G_G[:,m] = np.sum((out2['beta_seq'] - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse3_G_G[:,m] = np.sum((out3['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Errors for Submodel 2: gd vs. noisygd with Varying Sample Sizes\n",
    "In this subsection, we compare the errors of the following models as a function of the sample size:\n",
    "\n",
    "- **gd**: The non-noisy gradient descent model.\n",
    "- **noisygd**: The gradient descent model with different privacy parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 10\n",
    "lr = .5\n",
    "c0 = .5\n",
    "mu = np.array([.3, .5 ,.9])\n",
    "repetitions = 300\n",
    "sample_sizes = np.array(range(2000, 30001, 2000))\n",
    "\n",
    "rgt.seed(0)\n",
    "beta = np.ones(p)*(2*rgt.binomial(1, 0.5, size=p)-1)\n",
    "beta_true = np.insert(beta, 0, 1)\n",
    "beta_norm = beta_true.dot(beta_true)**0.5\n",
    "\n",
    "mean_mse_G_G = []\n",
    "mean_priv_mse1_G_G = []\n",
    "mean_priv_mse2_G_G = []\n",
    "mean_priv_mse3_G_G = []\n",
    "for n in sample_sizes:\n",
    "    errors_G_G = []\n",
    "    errors_priv1_G_G = []\n",
    "    errors_priv2_G_G = []\n",
    "    errors_priv3_G_G = []\n",
    "    T = int(np.ceil(np.log(n)))\n",
    "    B = (p + np.log(n))**0.5\n",
    "    robust = c0 * (n/(p+np.log(n)))**0.5\n",
    "    for m in range(repetitions):\n",
    "        rgt.seed(m+1)\n",
    "        X = rgt.normal(0, 1, size=(n,p))\n",
    "        Y = 1 + X.dot(beta) + rgt.normal(0,1, n)\n",
    "\n",
    "        model = Huber(X, Y)\n",
    "        out0 = model.gd(robust, lr=lr, max_niter=T)\n",
    "\n",
    "        priv_robust = c0 * mad(out0['residuals']) * (n*mu/(p+np.log(n)))**0.5\n",
    "        out1 = model.noisygd(priv_robust[0], lr=lr, B=B, mu=mu[0], T=T)\n",
    "        out2 = model.noisygd(priv_robust[1], lr=lr, B=B, mu=mu[1], T=T)\n",
    "        out3 = model.noisygd(priv_robust[2], lr=lr, B=B, mu=mu[2], T=T)\n",
    "\n",
    "        errors_G_G.append(np.sum((out0['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv1_G_G.append(np.sum((out1['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv2_G_G.append(np.sum((out2['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv3_G_G.append(np.sum((out3['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "    mean_mse_G_G.append(np.mean(errors_G_G))\n",
    "    mean_priv_mse1_G_G.append(np.mean(errors_priv1_G_G))\n",
    "    mean_priv_mse2_G_G.append(np.mean(errors_priv2_G_G))\n",
    "    mean_priv_mse3_G_G.append(np.mean(errors_priv3_G_G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodel 3: Data Generation\n",
    "In this submodel, we generate the data as follows:\n",
    "\n",
    "- The predictor matrix  X  follows a uniform distribution:  X ~ U[-1,1] .\n",
    "- The error term  ε  follows a t-distribution with 2.25 degrees of freedom:  ε ~ t_{2.25}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Errors for Submodel 3: gd vs. noisygd\n",
    "In this subsection, we compare the errors of the following models as a function of the number of iterations:\n",
    "\n",
    "- **gd**: The non-noisy gradient descent model.\n",
    "- **noisygd**: The gradient descent model with different privacy parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 4000, 10\n",
    "t_df = 2.25\n",
    "lr = .5\n",
    "c0 = .5\n",
    "mu = np.array([.3, .5 ,.9])\n",
    "repetitions = 300 \n",
    "\n",
    "rgt.seed(0) \n",
    "beta = np.ones(p)*(2*rgt.binomial(1, 0.5, size=p)-1)\n",
    "beta_true = np.insert(beta, 0, 1)\n",
    "beta_norm = beta_true.dot(beta_true)**0.5\n",
    " \n",
    "T = int(np.ceil(np.log(n))) \n",
    "B = np.sqrt(p)  # truncation parameter for noise Gaussian DP: Uniform design\n",
    "robust = c0 * (n/(p+np.log(n)))**0.5 \n",
    "\n",
    "mse_U_t = np.zeros([T+1, repetitions])\n",
    "priv_mse1_U_t = np.zeros([T+1, repetitions])\n",
    "priv_mse2_U_t = np.zeros([T+1, repetitions])\n",
    "priv_mse3_U_t = np.zeros([T+1, repetitions])\n",
    "for m in range(repetitions):\n",
    "    rgt.seed(m+1)\n",
    "    X = rgt.uniform(-1, 1, size=(n,p))\n",
    "    Y = 1 + X.dot(beta) + rgt.standard_t(t_df, n)\n",
    "    \n",
    "    model = Huber(X, Y)\n",
    "    out0 = model.gd(robust, lr=lr, max_niter=T)\n",
    "\n",
    "    priv_robust = c0 * mad(out0['residuals']) * (n*mu/(p+np.log(n)))**0.5\n",
    "    out1 = model.noisygd(priv_robust[0], lr=lr, B=B, mu=mu[0], T=T)\n",
    "    out2 = model.noisygd(priv_robust[1], lr=lr, B=B, mu=mu[1], T=T)\n",
    "    out3 = model.noisygd(priv_robust[2], lr=lr, B=B, mu=mu[2], T=T)\n",
    "    \n",
    "    mse_U_t[:,m] = np.sum((out0['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse1_U_t[:,m] = np.sum((out1['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse2_U_t[:,m] = np.sum((out2['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse3_U_t[:,m] = np.sum((out3['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Errors for Submodel 3: gd vs. noisygd with Varying Sample Sizes\n",
    "In this subsection, we compare the errors of the following models as a function of the sample size:\n",
    "\n",
    "- **gd**: The non-noisy gradient descent model.\n",
    "- **noisygd**: The gradient descent model with different privacy parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 10\n",
    "t_df = 2.25\n",
    "lr = .5\n",
    "c0 = .5\n",
    "mu = np.array([.3, .5 ,.9])\n",
    "repetitions = 300\n",
    "sample_sizes = np.array(range(2000, 30001, 2000))\n",
    "\n",
    "rgt.seed(0)\n",
    "beta = np.ones(p)*(2*rgt.binomial(1, 0.5, size=p)-1)\n",
    "beta_true = np.insert(beta, 0, 1)\n",
    "beta_norm = beta_true.dot(beta_true)**0.5\n",
    "\n",
    "mean_mse_U_t = []\n",
    "mean_priv_mse1_U_t = []\n",
    "mean_priv_mse2_U_t = []\n",
    "mean_priv_mse3_U_t = []\n",
    "for n in sample_sizes:\n",
    "    errors_U_t = []\n",
    "    errors_priv1_U_t = []\n",
    "    errors_priv2_U_t = []\n",
    "    errors_priv3_U_t = []\n",
    "    T = int(np.ceil(np.log(n)))\n",
    "    B = np.sqrt(p)\n",
    "    robust = c0 * (n/(p+np.log(n)))**0.5\n",
    "    for m in range(repetitions):\n",
    "        rgt.seed(m+1)\n",
    "        X = rgt.uniform(-1, 1, size=(n,p))\n",
    "        Y = 1 + X.dot(beta) + rgt.standard_t(t_df, n)\n",
    "\n",
    "        model = Huber(X, Y)\n",
    "        out0 = model.gd(robust, lr=lr, max_niter=T)\n",
    "\n",
    "        priv_robust = c0 * mad(out0['residuals']) * (n*mu/(p+np.log(n)))**0.5\n",
    "        out1 = model.noisygd(priv_robust[0], lr=lr, B=B, mu=mu[0], T=T)\n",
    "        out2 = model.noisygd(priv_robust[1], lr=lr, B=B, mu=mu[1], T=T)\n",
    "        out3 = model.noisygd(priv_robust[2], lr=lr, B=B, mu=mu[2], T=T)\n",
    "\n",
    "        errors_U_t.append(np.sum((out0['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv1_U_t.append(np.sum((out1['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv2_U_t.append(np.sum((out2['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv3_U_t.append(np.sum((out3['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "    mean_mse_U_t.append(np.mean(errors_U_t))\n",
    "    mean_priv_mse1_U_t.append(np.mean(errors_priv1_U_t))\n",
    "    mean_priv_mse2_U_t.append(np.mean(errors_priv2_U_t))\n",
    "    mean_priv_mse3_U_t.append(np.mean(errors_priv3_U_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodel 4: Data Generation\n",
    "In this submodel, we generate the data as follows:\n",
    "\n",
    "- The predictor matrix  X  follows a uniform distribution:  X ~ U[-1,1] .\n",
    "- The error term  ε  follows a normal distribution:  ε ~ N(0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Errors for Submodel 4: gd vs. noisygd\n",
    "In this subsection, we compare the errors of the following models as a function of the number of iterations:\n",
    "\n",
    "- **gd**: The non-noisy gradient descent model.\n",
    "- **noisygd**: The gradient descent model with different privacy parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 4000, 10\n",
    "lr = .5\n",
    "c_0 = .5\n",
    "mu = np.array([.3, .5 ,.9])\n",
    "repetitions = 300\n",
    "\n",
    "rgt.seed(0) \n",
    "beta = np.ones(p)*(2*rgt.binomial(1, 0.5, size=p)-1)\n",
    "beta_true = np.insert(beta, 0, 1)\n",
    "beta_norm = beta_true.dot(beta_true)**0.5 \n",
    " \n",
    "T = int(np.ceil(np.log(n))) \n",
    "B = np.sqrt(p) \n",
    "robust = c0 * (n/(p+np.log(n)))**0.5 \n",
    "\n",
    "mse_U_G = np.zeros([T+1, repetitions])\n",
    "priv_mse1_U_G = np.zeros([T+1, repetitions])\n",
    "priv_mse2_U_G = np.zeros([T+1, repetitions])\n",
    "priv_mse3_U_G = np.zeros([T+1, repetitions])\n",
    "for m in range(repetitions):\n",
    "    rgt.seed(m+1)\n",
    "    X = rgt.uniform(-1, 1, size=(n,p))\n",
    "    Y = 1 + X.dot(beta) + rgt.normal(0,1, n)\n",
    "    \n",
    "    model = Huber(X, Y)\n",
    "    out0 = model.gd(robust, lr=lr, max_niter=T)\n",
    "\n",
    "    priv_robust = c0 * mad(out0['residuals']) * (n*mu/(p+np.log(n)))**0.5\n",
    "    out1 = model.noisygd(priv_robust[0], lr=lr, B=B, mu=mu[0], T=T)\n",
    "    out2 = model.noisygd(priv_robust[1], lr=lr, B=B, mu=mu[1], T=T)\n",
    "    out3 = model.noisygd(priv_robust[2], lr=lr, B=B, mu=mu[2], T=T)\n",
    "    \n",
    "    mse_U_G[:,m] = np.sum((out0['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse1_U_G[:,m] = np.sum((out1['beta_seq']  - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse2_U_G[:,m] = np.sum((out2['beta_seq'] - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    priv_mse3_U_G[:,m] = np.sum((out3['beta_seq'] - beta_true[:,None])**2, axis=0)**0.5  / beta_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Errors for Submodel 4: gd vs. noisygd with Varying Sample Sizes\n",
    "In this subsection, we compare the errors of the following models as a function of the sample size:\n",
    "\n",
    "- **gd**: The non-noisy gradient descent model.\n",
    "- **noisygd**: The gradient descent model with different privacy parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 10\n",
    "lr = .5\n",
    "c0 = .5\n",
    "mu = np.array([.3, .5 ,.9])\n",
    "repetitions = 300\n",
    "sample_sizes = np.array(range(2000, 30001, 2000)) \n",
    "\n",
    "rgt.seed(0)\n",
    "beta = np.ones(p)*(2*rgt.binomial(1, 0.5, size=p)-1)\n",
    "beta_true = np.insert(beta, 0, 1)\n",
    "beta_norm = beta_true.dot(beta_true)**0.5 \n",
    "\n",
    "mean_mse_U_G = []\n",
    "mean_priv_mse1_U_G = []\n",
    "mean_priv_mse2_U_G = []\n",
    "mean_priv_mse3_U_G = []\n",
    "for n in sample_sizes:\n",
    "    errors_U_G = []\n",
    "    errors_priv1_U_G = []\n",
    "    errors_priv2_U_G = []\n",
    "    errors_priv3_U_G = []\n",
    "    T = int(np.ceil(np.log(n)))\n",
    "    B = np.sqrt(p)\n",
    "    robust = c0 * (n/(p+np.log(n)))**0.5\n",
    "    for m in range(repetitions):\n",
    "        rgt.seed(m+1)\n",
    "        X = rgt.uniform(-1, 1, size=(n,p))\n",
    "        Y = 1 + X.dot(beta) + rgt.normal(0,1, n)\n",
    "\n",
    "        model = Huber(X, Y)\n",
    "        out0 = model.gd(robust, lr=lr, max_niter=T)\n",
    "\n",
    "        priv_robust = c0 * mad(out0['residuals']) * (n*mu/(p+np.log(n)))**0.5\n",
    "        out1 = model.noisygd(priv_robust[0], lr=lr, B=B, mu=mu[0], T=T)\n",
    "        out2 = model.noisygd(priv_robust[1], lr=lr, B=B, mu=mu[1], T=T)\n",
    "        out3 = model.noisygd(priv_robust[2], lr=lr, B=B, mu=mu[2], T=T)\n",
    "\n",
    "        errors_U_G.append(np.sum((out0['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv1_U_G.append(np.sum((out1['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv2_U_G.append(np.sum((out2['beta'] - beta_true)**2)**0.5  / beta_norm)\n",
    "        errors_priv3_U_G.append(np.sum((out3['beta']  - beta_true)**2)**0.5  / beta_norm)\n",
    "    mean_mse_U_G.append(np.mean(errors_U_G))\n",
    "    mean_priv_mse1_U_G.append(np.mean(errors_priv1_U_G))\n",
    "    mean_priv_mse2_U_G.append(np.mean(errors_priv2_U_G))\n",
    "    mean_priv_mse3_U_G.append(np.mean(errors_priv3_U_G))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
