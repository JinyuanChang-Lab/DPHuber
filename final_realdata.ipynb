{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data Analysis\n",
    "\n",
    "In this section, we import necessary libraries and tools to perform real data analysis. \n",
    "\n",
    "- **Libraries and Tools:**\n",
    "  - `final_util`: A custom utility module for specific model functions.\n",
    "  - `numpy`: For numerical computations and array manipulations.\n",
    "  - `pandas`: For handling and preprocessing tabular data.\n",
    "  - `os`: For interacting with the operating system to manage files and directories.\n",
    "  - `sklearn.preprocessing.StandardScaler`: For standardizing features by removing the mean and scaling to unit variance.\n",
    "  - `matplotlib.pyplot`: For data visualization and plotting.\n",
    "  - `sklearn.model_selection.KFold`: For implementing k-fold cross-validation to evaluate model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_util import * \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Dimensional Real Dataset\n",
    "\n",
    "In this section, we analyze a low-dimensional real-world dataset: **housing price data**. This dataset contains information on housing features and their corresponding prices, making it suitable for regression analysis in a low-dimensional setting.\n",
    "\n",
    "- **Dataset Overview**:\n",
    "  - The dataset includes various features related to houses, such as median income,housing median age,total population,number of households,and the total number of rooms across all houses within a block group.\n",
    "  - The target variable is the logarithm of the median house value of each block group.\n",
    "\n",
    "- **Download Address**:\n",
    "  The dataset can be downloaded from the following link: [Housing Price Data](http://lib.stat.cmu.edu/datasets/houses.zip) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In this step, we process the housing price dataset for analysis:\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - Read the dataset from the  local file.\n",
    "   - The dataset used in this analysis is derived from the original **cadata.txt** file. The **cadata_new.txt** file has been preprocessed by removing the textual descriptions from the original dataset, leaving only the numerical data.   \n",
    "   - Extract the predictor variables (features) and response variable (median house value).\n",
    "\n",
    "2. **Log Transformation**:\n",
    "   - Apply a logarithmic transformation to the response variable (median house value).\n",
    "\n",
    "3. **Feature Standardization**:\n",
    "   - Standardize the predictor variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data ###\n",
    "current_dir = os.getcwd()\n",
    "file_path = os.path.join(current_dir, 'realdata/lowdim_house_data/cadata_new.txt')\n",
    "data_org  = np.loadtxt(file_path)\n",
    "data_house_org = pd.DataFrame(data_org,columns=['median_house_value','median_income','housing_median_age','total_rooms','total_bedrooms','population','households','latitude','longitude'])\n",
    "data_house = data_house_org[['median_house_value','median_income','housing_median_age','population','households','total_rooms']]\n",
    "\n",
    "Y_house = np.array(np.log(data_house['median_house_value']))\n",
    "X_house = data_house.iloc[:,1:] # p=5\n",
    "## standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_house = pd.DataFrame(scaler.fit_transform(X_house),columns=X_house.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Compare the ε-GDP Huber vs. Non-Private Huber\n",
    "\n",
    "In this section, we compare the performance of `noisygd` with `gd`. The ε-GDP  Huber regression introduces privacy-preserving noise into the model, while the non-private Huber regression does not.\n",
    "\n",
    "We will evaluate both models in terms of:\n",
    "1. **Error comparison**: Measure the prediction errors for both models using the same dataset.\n",
    "2. **Impact of privacy**: Analyze how the introduction of noise in the ε-GDP Huber regression affects the model's performance, particularly as the privacy parameter (ε) varies, in comparison to the benchmark, which is the performance of the `gd` applied to the entire dataset without any privacy constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "800\n",
      "1200\n",
      "1600\n",
      "2000\n",
      "2400\n",
      "2800\n",
      "3200\n",
      "3600\n",
      "4000\n",
      "4400\n",
      "4800\n",
      "5200\n",
      "5600\n",
      "6000\n",
      "6400\n",
      "6800\n",
      "7200\n",
      "7600\n",
      "8000\n",
      "8400\n",
      "8800\n",
      "9200\n",
      "9600\n",
      "10000\n",
      "10400\n",
      "10800\n",
      "11200\n",
      "11600\n",
      "12000\n",
      "12400\n",
      "12800\n",
      "13200\n",
      "13600\n",
      "14000\n",
      "14400\n",
      "14800\n",
      "15200\n",
      "15600\n",
      "16000\n",
      "16400\n",
      "16800\n",
      "17200\n",
      "17600\n",
      "18000\n",
      "18400\n",
      "18800\n",
      "19200\n",
      "19600\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "sequence = np.arange(400, 20001, 400)\n",
    "Y_house_copy = np.array(Y_house.copy()).reshape(-1)\n",
    "X_house_copy = np.array(X_house.copy())\n",
    "n = X_house_copy.shape[0]\n",
    "p = X_house_copy.shape[1]\n",
    " \n",
    "#### Comparison ####\n",
    "mu = np.array([.3, .5, .9])\n",
    "result_mse = []\n",
    "result_priv1_mse = []\n",
    "result_priv2_mse = []\n",
    "result_priv3_mse = []\n",
    "\n",
    "for index in range(len(sequence)):\n",
    "    rgt.seed(index+1)\n",
    "    numm = sequence[index]\n",
    "    print(numm)\n",
    "    random_rows = np.random.choice(n, numm, replace=False)\n",
    "    X_sample = X_house_copy[random_rows] \n",
    "    Y_sample = Y_house_copy[random_rows] \n",
    "\n",
    "    B = (p + np.log(numm))**0.5\n",
    "    T = int(np.ceil(np.log(numm)))\n",
    "    result_mse_temp = []\n",
    "    result_priv1_mse_temp = []\n",
    "    result_priv2_mse_temp = []\n",
    "    result_priv3_mse_temp = []\n",
    "    \n",
    "    for rep in range(300):\n",
    "        rgt.seed(rep + 1)\n",
    "        train_idx = np.random.choice(numm, size=int(0.8 * numm), replace=False)\n",
    "        test_idx = np.setdiff1d(np.arange(numm), train_idx)\n",
    "        \n",
    "        # Split into 80% training and 20% testing \n",
    "        X_train, X_test = X_sample[train_idx], X_sample[test_idx]\n",
    "        Y_train, Y_test = Y_sample[train_idx], Y_sample[test_idx]\n",
    "        \n",
    "        X_test_new = np.concatenate([np.ones((len(X_test), 1)), X_test], axis=1)\n",
    "        ## Non-private Huber\n",
    "        train_size = 0.8 * numm\n",
    "        model_huber_temp = Huber(X_train, Y_train)\n",
    "        out_huber_temp = model_huber_temp.gd(robust=.5 * (train_size/(p+np.log(train_size)))**0.5, lr=.5, max_niter=T)\n",
    "        Y_pred = X_test_new @ out_huber_temp['beta']\n",
    "        mse_huber = np.mean((Y_test - Y_pred) ** 2)\n",
    "        \n",
    "        ## ε-GDP Huber\n",
    "        priv_robust = .5 * mad(out_huber_temp['residuals']) * (train_size * mu / (p + np.log(train_size)))**0.5\n",
    "        out_priv1_temp = model_huber_temp.noisygd(priv_robust[0], lr=.5, B=B, mu=mu[0], T=T)\n",
    "        out_priv2_temp = model_huber_temp.noisygd(priv_robust[1], lr=.5, B=B, mu=mu[1], T=T)\n",
    "        out_priv3_temp = model_huber_temp.noisygd(priv_robust[2], lr=.5, B=B, mu=mu[2], T=T)\n",
    "        \n",
    "        # Compute MSE for private versions\n",
    "        Y_priv1_pred = X_test_new @ out_priv1_temp['beta']\n",
    "        Y_priv2_pred = X_test_new @ out_priv2_temp['beta']\n",
    "        Y_priv3_pred = X_test_new @ out_priv3_temp['beta']\n",
    "        \n",
    "        mse_priv1 = np.mean((Y_test - Y_priv1_pred) ** 2)\n",
    "        mse_priv2 = np.mean((Y_test - Y_priv2_pred) ** 2)\n",
    "        mse_priv3 = np.mean((Y_test - Y_priv3_pred) ** 2)\n",
    "        \n",
    "        result_mse_temp.append(mse_huber)\n",
    "        result_priv1_mse_temp.append(mse_priv1)\n",
    "        result_priv2_mse_temp.append(mse_priv2)\n",
    "        result_priv3_mse_temp.append(mse_priv3)\n",
    "    \n",
    "    result_mse.append(np.mean(result_mse_temp))\n",
    "    result_priv1_mse.append(np.mean(result_priv1_mse_temp))\n",
    "    result_priv2_mse.append(np.mean(result_priv2_mse_temp))\n",
    "    result_priv3_mse.append(np.mean(result_priv3_mse_temp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Dimensional Real Dataset\n",
    "\n",
    "In this section, we analyze a high-dimensional real-world dataset: **Communities and Crime**. This dataset contains information about various features of communities and their corresponding crime rates.\n",
    "\n",
    "- **Dataset Overview**:\n",
    "  - The dataset includes socio-economic and crime data from U.S. communities.  There are 125 features with 18 potential target variables (e.g., various crime rates). \n",
    "  - The target variable in our analysis is the logarithm of **ViolentCrimesPerPop**(total number of violent crimes per 100,000 population).\n",
    "\n",
    "- **Download Address**:\n",
    "  The dataset can be downloaded from the following link: [Communities and Crime Dataset](https://archive.ics.uci.edu/dataset/211/communities+and+crime+unnormalized) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing for Communities and Crime Dataset\n",
    "\n",
    "In this step, we process the dataset by selecting **ViolentCrimesPerPop** as the response variable. We handle multicollinearity by removing the following variables, which are linearly correlated:\n",
    "\n",
    "- `pctUrban`: Percentage of urban population.\n",
    "- `OwnOccQrange`: Difference between the upper and lower quartiles of home ownership.\n",
    "- `RentQrange`: Difference between high and low rent quartiles.\n",
    "\n",
    "After removing these correlated variables, we delete columns with missing data and remove rows where the response variable is missing, leaving 1994 rows and 99 columns for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data ###\n",
    "current_dir = os.getcwd()\n",
    "file_path = os.path.join(current_dir, 'realdata/highdim/Data.txt')\n",
    "data_org = pd.read_csv(file_path, delimiter=',', header=None)\n",
    "colnames_comm = [\n",
    "    \"communityname\", \"state\", \"countyCode\", \"communityCode\", \"fold\", \"population\", \"householdsize\",\n",
    "    \"racepctblack\", \"racePctWhite\", \"racePctAsian\", \"racePctHisp\", \"agePct12t21\", \"agePct12t29\",\n",
    "    \"agePct16t24\", \"agePct65up\", \"numbUrban\", \"pctUrban\", \"medIncome\", \"pctWWage\", \"pctWFarmSelf\",\n",
    "    \"pctWInvInc\", \"pctWSocSec\", \"pctWPubAsst\", \"pctWRetire\", \"medFamInc\", \"perCapInc\", \"whitePerCap\",\n",
    "    \"blackPerCap\", \"indianPerCap\", \"AsianPerCap\", \"OtherPerCap\", \"HispPerCap\", \"NumUnderPov\",\n",
    "    \"PctPopUnderPov\", \"PctLess9thGrade\", \"PctNotHSGrad\", \"PctBSorMore\", \"PctUnemployed\", \"PctEmploy\",\n",
    "    \"PctEmplManu\", \"PctEmplProfServ\", \"PctOccupManu\", \"PctOccupMgmtProf\", \"MalePctDivorce\",\n",
    "    \"MalePctNevMarr\", \"FemalePctDiv\", \"TotalPctDiv\", \"PersPerFam\", \"PctFam2Par\", \"PctKids2Par\",\n",
    "    \"PctYoungKids2Par\", \"PctTeen2Par\", \"PctWorkMomYoungKids\", \"PctWorkMom\", \"NumKidsBornNeverMar\",\n",
    "    \"PctKidsBornNeverMar\", \"NumImmig\", \"PctImmigRecent\", \"PctImmigRec5\", \"PctImmigRec8\", \"PctImmigRec10\",\n",
    "    \"PctRecentImmig\", \"PctRecImmig5\", \"PctRecImmig8\", \"PctRecImmig10\", \"PctSpeakEnglOnly\",\n",
    "    \"PctNotSpeakEnglWell\", \"PctLargHouseFam\", \"PctLargHouseOccup\", \"PersPerOccupHous\", \"PersPerOwnOccHous\",\n",
    "    \"PersPerRentOccHous\", \"PctPersOwnOccup\", \"PctPersDenseHous\", \"PctHousLess3BR\", \"MedNumBR\", \"HousVacant\",\n",
    "    \"PctHousOccup\", \"PctHousOwnOcc\", \"PctVacantBoarded\", \"PctVacMore6Mos\", \"MedYrHousBuilt\", \"PctHousNoPhone\",\n",
    "    \"PctWOFullPlumb\", \"OwnOccLowQuart\", \"OwnOccMedVal\", \"OwnOccHiQuart\", \"OwnOccQrange\", \"RentLowQ\",\n",
    "    \"RentMedian\", \"RentHighQ\", \"RentQrange\", \"MedRent\", \"MedRentPctHousInc\", \"MedOwnCostPctInc\",\n",
    "    \"MedOwnCostPctIncNoMtg\", \"NumInShelters\", \"NumStreet\", \"PctForeignBorn\", \"PctBornSameState\",\n",
    "    \"PctSameHouse85\", \"PctSameCity85\", \"PctSameState85\", \"LemasSwornFT\", \"LemasSwFTPerPop\",\n",
    "    \"LemasSwFTFieldOps\", \"LemasSwFTFieldPerPop\", \"LemasTotalReq\", \"LemasTotReqPerPop\", \"PolicReqPerOffic\",\n",
    "    \"PolicPerPop\", \"RacialMatchCommPol\", \"PctPolicWhite\", \"PctPolicBlack\", \"PctPolicHisp\", \"PctPolicAsian\",\n",
    "    \"PctPolicMinor\", \"OfficAssgnDrugUnits\", \"NumKindsDrugsSeiz\", \"PolicAveOTWorked\", \"LandArea\", \"PopDens\",\n",
    "    \"PctUsePubTrans\", \"PolicCars\", \"PolicOperBudg\", \"LemasPctPolicOnPatr\", \"LemasGangUnitDeploy\",\n",
    "    \"LemasPctOfficDrugUn\", \"PolicBudgPerPop\", \"murders\", \"murdPerPop\", \"rapes\", \"rapesPerPop\",\n",
    "    \"robberies\", \"robbbPerPop\", \"assaults\", \"assaultPerPop\", \"burglaries\", \"burglPerPop\", \"larcenies\",\n",
    "    \"larcPerPop\", \"autoTheft\", \"autoTheftPerPop\", \"arsons\", \"arsonsPerPop\", \"ViolentCrimesPerPop\",\n",
    "    \"nonViolPerPop\"\n",
    "]\n",
    "data_org.columns = colnames_comm\n",
    "data_org_new = data_org[[  \"fold\", \"population\", \"householdsize\",\n",
    "\"racepctblack\", \"racePctWhite\", \"racePctAsian\", \"racePctHisp\", \"agePct12t21\", \"agePct12t29\",\n",
    "\"agePct16t24\", \"agePct65up\", \"numbUrban\", \"pctUrban\", \"medIncome\", \"pctWWage\", \"pctWFarmSelf\",\n",
    "\"pctWInvInc\", \"pctWSocSec\", \"pctWPubAsst\", \"pctWRetire\", \"medFamInc\", \"perCapInc\", \"whitePerCap\",\n",
    "\"blackPerCap\", \"indianPerCap\", \"AsianPerCap\", \"OtherPerCap\", \"HispPerCap\", \"NumUnderPov\",\n",
    "\"PctPopUnderPov\", \"PctLess9thGrade\", \"PctNotHSGrad\", \"PctBSorMore\", \"PctUnemployed\", \"PctEmploy\",\n",
    "\"PctEmplManu\", \"PctEmplProfServ\", \"PctOccupManu\", \"PctOccupMgmtProf\", \"MalePctDivorce\",\n",
    "\"MalePctNevMarr\", \"FemalePctDiv\", \"TotalPctDiv\", \"PersPerFam\", \"PctFam2Par\", \"PctKids2Par\",\n",
    "\"PctYoungKids2Par\", \"PctTeen2Par\", \"PctWorkMomYoungKids\", \"PctWorkMom\", \"NumKidsBornNeverMar\",\n",
    "\"PctKidsBornNeverMar\", \"NumImmig\", \"PctImmigRecent\", \"PctImmigRec5\", \"PctImmigRec8\", \"PctImmigRec10\",\n",
    "\"PctRecentImmig\", \"PctRecImmig5\", \"PctRecImmig8\", \"PctRecImmig10\", \"PctSpeakEnglOnly\",\n",
    "\"PctNotSpeakEnglWell\", \"PctLargHouseFam\", \"PctLargHouseOccup\", \"PersPerOccupHous\", \"PersPerOwnOccHous\",\n",
    "\"PersPerRentOccHous\", \"PctPersOwnOccup\", \"PctPersDenseHous\", \"PctHousLess3BR\", \"MedNumBR\", \"HousVacant\",\n",
    "\"PctHousOccup\", \"PctHousOwnOcc\", \"PctVacantBoarded\", \"PctVacMore6Mos\", \"MedYrHousBuilt\", \"PctHousNoPhone\",\n",
    "\"PctWOFullPlumb\", \"OwnOccLowQuart\", \"OwnOccMedVal\", \"OwnOccHiQuart\", \"OwnOccQrange\", \"RentLowQ\",\n",
    "\"RentMedian\", \"RentHighQ\", \"RentQrange\", \"MedRent\", \"MedRentPctHousInc\", \"MedOwnCostPctInc\",\n",
    "\"MedOwnCostPctIncNoMtg\", \"NumInShelters\", \"NumStreet\", \"PctForeignBorn\", \"PctBornSameState\",\n",
    "\"PctSameHouse85\", \"PctSameCity85\", \"PctSameState85\", \"LemasSwornFT\", \"LemasSwFTPerPop\",\n",
    "\"LemasSwFTFieldOps\", \"LemasSwFTFieldPerPop\", \"LemasTotalReq\", \"LemasTotReqPerPop\", \"PolicReqPerOffic\",\n",
    "\"PolicPerPop\", \"RacialMatchCommPol\", \"PctPolicWhite\", \"PctPolicBlack\", \"PctPolicHisp\", \"PctPolicAsian\",\n",
    "\"PctPolicMinor\", \"OfficAssgnDrugUnits\", \"NumKindsDrugsSeiz\", \"PolicAveOTWorked\", \"LandArea\", \"PopDens\",\n",
    "\"PctUsePubTrans\", \"PolicCars\", \"PolicOperBudg\", \"LemasPctPolicOnPatr\", \"LemasGangUnitDeploy\",\n",
    "\"LemasPctOfficDrugUn\", \"PolicBudgPerPop\",  \"ViolentCrimesPerPop\" ]]\n",
    "#sum_ab = data_org_new['RentHighQ']-data_org_new['RentLowQ']- data_org_new['RentQrange'] ## all 0\n",
    "multicollinearity = ['pctUrban','OwnOccQrange','RentQrange']\n",
    "# pctUrban = 100*numbUrban / population\n",
    "# OwnOccQrange = OwnOccHiQuart - OwnOccLowQuart\n",
    "# RentQrange = RentHighQ - RentLowQ\n",
    "missing_col = data_org_new.columns[data_org_new.apply(lambda col: col.astype(str).str.contains('\\\\?').any())]\n",
    "missing_variable_name = ['OtherPerCap', 'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps',\n",
    "       'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop',\n",
    "       'PolicReqPerOffic', 'PolicPerPop', 'RacialMatchCommPol',\n",
    "       'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp', 'PctPolicAsian',\n",
    "       'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz',\n",
    "       'PolicAveOTWorked', 'PolicCars', 'PolicOperBudg', 'LemasPctPolicOnPatr',\n",
    "       'LemasGangUnitDeploy', 'PolicBudgPerPop']\n",
    "## with missing values\n",
    " \n",
    "columns_to_drop = multicollinearity + missing_variable_name\n",
    " \n",
    "data_COMM = data_org_new.drop(columns=columns_to_drop)\n",
    "data_COMM = data_COMM[data_COMM['ViolentCrimesPerPop'] != '?'] # Remove rows where the response variable is missing.\n",
    "data_COMM = data_COMM.reset_index(drop=True)\n",
    "\n",
    "Y_data_COMM = data_COMM['ViolentCrimesPerPop']\n",
    "X_data_COMM = data_COMM.drop(columns='ViolentCrimesPerPop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Standardization and Removal of Constant Variables\n",
    "\n",
    "In this step:\n",
    "\n",
    "1. **Standardization of the Response Variable**: The response variable **ViolentCrimesPerPop** is standardized using **StandardScaler**, which scales the variable to have zero mean and unit variance.\n",
    "\n",
    "2. **Standardization of Covariates**: All covariates are also standardized using **StandardScaler** to ensure consistency across variables with different units of measurement.\n",
    "\n",
    "3. **Removal of Intercept Term**: The intercept term is removed to facilitate comparison with the **sparse DP LS**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "Y_data_COMM = Y_data_COMM.values.reshape(-1, 1) \n",
    "scaler = StandardScaler()\n",
    "Y_COMM = scaler.fit_transform(Y_data_COMM) \n",
    "Y_COMM = pd.Series(Y_COMM.flatten()) \n",
    "\n",
    "scaler_x1 = StandardScaler()\n",
    "X_COMM = pd.DataFrame(scaler_x1.fit_transform(X_data_COMM), columns=X_data_COMM.columns)\n",
    "X_COMM = X_COMM.iloc[:,1:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### s Selection\n",
    "\n",
    "To estimate the sparsity parameter **s**, we use the `noisygd_highdim` method with **10-fold Cross Validation**. The value of **s** is chosen by minimizing the **Mean Squared Error (MSE)**, ensuring that the sparsity level selected leads to the best predictive performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing s = 1\n",
      "Processing s = 2\n",
      "Processing s = 3\n",
      "Processing s = 4\n",
      "Processing s = 5\n",
      "Processing s = 6\n",
      "Processing s = 7\n",
      "Processing s = 8\n",
      "Processing s = 9\n",
      "Processing s = 10\n",
      "Optimal s = 3, Final MSE = [0.43433241]\n"
     ]
    }
   ],
   "source": [
    "p = X_COMM.shape[1]\n",
    "numm = 1994  \n",
    "mu = 0.5 \n",
    "delta = 0.01\n",
    "lr = 0.1 \n",
    "s_list = list(range(1, 11))\n",
    "re_matrix = np.zeros((10, 1)) \n",
    "n_1 = 200 \n",
    "n_2 = int(0.9*numm - n_1)\n",
    "T = int(np.ceil(np.log(n_2)))  ## number of iterations\n",
    "B_huber = .4 * (np.log(p) + np.log(n_2))**0.5  # truncation parameter\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)  # 10-fold cross-validation\n",
    "\n",
    "for s in s_list:\n",
    "    print(f\"Processing s = {s}\")\n",
    "    rgt.seed(s+1)\n",
    "    robust_noiseless = (n_1 / (s * np.log(p) + np.log(n_1)))**0.5  # robust parameter for initial estimation\n",
    "    c_0 = 0.2  # constant for DP robust parameter\n",
    "    robust = c_0 * (n_2 / (s * np.log(p) + np.log(n_2)))**0.5\n",
    "    robust_noise = c_0 * (n_2 * mu / (s * np.log(p) + np.log(n_2)))**0.5\n",
    "    \n",
    "    robust_low1 = 0.5 * (n_2 * mu / (s + 1 + np.log(n_2)))**0.5\n",
    "    robust_low2 = 0.5 * (n_2   / (s + 1 + np.log(n_2)))**0.5\n",
    "    \n",
    "    result_mse = []  # Store MSE for each fold\n",
    "\n",
    "    for train_index, test_index in kf.split(X_COMM):  # Split data into 10 folds\n",
    "        \n",
    "        # Split the data into training and testing sets for each fold\n",
    "        X_train, X_test = X_COMM.iloc[train_index], X_COMM.iloc[test_index]\n",
    "        Y_train, Y_test = Y_COMM.iloc[train_index], Y_COMM.iloc[test_index]\n",
    "        Y_train = pd.to_numeric(Y_train)\n",
    "        Y_test = pd.to_numeric(Y_test)\n",
    "\n",
    "        # Subsample for initial estimation\n",
    "        random_rows = np.random.choice(len(X_train), size=n_1, replace=False)\n",
    "        X_subsample = X_train.iloc[random_rows]\n",
    "        Y_subsample = Y_train.iloc[random_rows]\n",
    "        \n",
    "        # Initial model fit with Huber regression\n",
    "        model_sub = Huber(X_subsample, Y_subsample, intercept=True)\n",
    "        initial = model_sub.gd_highdim(lr=0.5, T=4, s=s, tau=None, robust=robust_noiseless, beta0=np.array([]), \n",
    "                                       standardize=False)\n",
    "        \n",
    "        # Fit the model on the remaining data (rest of the data)\n",
    "        # Use .iloc to ensure correct row selection using integer-based indexing\n",
    "        X_rest = X_train.drop(index=X_train.index[random_rows])\n",
    "        Y_rest = Y_train.drop(index=Y_train.index[random_rows])\n",
    "        beta0 = initial['beta']\n",
    "        \n",
    "        model = Huber(X_rest, Y_rest, intercept=True)\n",
    "        out_Huber_noise_new = model.noisygd_highdim(mu=mu, T=T, delta=delta, lr=lr, beta0=beta0, s=s, \n",
    "                                                       robust_low1=robust_low1, robust_low2=robust_low2, robust_high1=robust, \n",
    "                                                       robust_high2=robust_noise, B_high=B_huber, standardize=False)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        X_test = np.concatenate([np.ones((len(X_test), 1)), X_test], axis=1)\n",
    "        Y_test_predict = X_test @ out_Huber_noise_new['beta1']\n",
    "        \n",
    "        # Calculate Mean Squared Error (MSE) for this fold\n",
    "        mse = np.mean((Y_test_predict - Y_test) ** 2)\n",
    "        result_mse.append(mse)\n",
    "\n",
    "    # Average the MSE across all folds\n",
    "    re_matrix[s-1, 0] = np.mean(result_mse)\n",
    "\n",
    "# Find the best parameter s (with minimum MSE)\n",
    "min_index = np.argmin(re_matrix[:, 0])\n",
    "final_re = re_matrix[min_index]\n",
    "print(f\"Optimal s = {min_index + 1}, Final MSE = {final_re}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of sparse DP Huber and sparse DP LS Methods\n",
    "\n",
    "Using **s = 3**, estimated through cross-validation, we compare the prediction errors of the **sparse DP Huber**(`noisygd_highdim`) and **sparse DP LS**(`noisygd_ls`) methods. The results from **sparse Huber**(`gd_highdim`,without noise) serve as the **benchmark** for reference, allowing us to evaluate how the introduction of noise and privacy constraints impacts the performance of the other methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    }
   ],
   "source": [
    "n = 1994\n",
    "s = 3\n",
    "p = X_COMM.shape[1]  \n",
    "n_1 = 200 \n",
    "mu = 0.5 \n",
    "delta = 0.01\n",
    "lr = 0.1   \n",
    "robust_noiseless = (n_1/(s*np.log(p)+np.log(n_1)))**0.5\n",
    "sequence = np.arange(500,1901,100)\n",
    "result_Huber = []\n",
    "result_Huber_noise = []\n",
    "result_ls_noise = []\n",
    "for index in range(len(sequence)):\n",
    "    rgt.seed(index+1)\n",
    "    numm = sequence[index]\n",
    "    print(numm)\n",
    "    \n",
    "    random_rows1 = np.random.choice(n, numm, replace=False)\n",
    "    Y_COMM_sample = Y_COMM.iloc[random_rows1]\n",
    "    Y_COMM_sample = Y_COMM_sample.reset_index(drop=True)\n",
    "    X_COMM_sample = X_COMM.iloc[random_rows1]\n",
    "    X_COMM_sample = X_COMM_sample.reset_index(drop=True)\n",
    "    \n",
    "    n_2 = int(0.8*numm) - n_1 \n",
    "    B_huber = .4*(np.log(p) + np.log(n_2))**0.5  #  ## truncation parameter \n",
    "    T =   int(np.ceil(np.log(n_2)))  ## number of iterations\n",
    "    robust = .2 * (n_2/(s*np.log(p)+np.log(n_2)))**0.5\n",
    "    robust_noise = .2 * (n_2*mu/(s*np.log(p)+np.log(n_2)))**0.5 \n",
    "    robust_low1 = .5*(n_2 /(s+1+np.log(n_2)))**0.5\n",
    "    robust_low2 = .5*(n_2*mu/(s+1+np.log(n_2)))**0.5\n",
    "    \n",
    "    \n",
    "    M=300 \n",
    "    HD_Huber = np.zeros(M)\n",
    "    HD_Huber_noise = np.zeros(M)\n",
    "    HD_ls_noise = np.zeros(M)\n",
    "    for rep in list(range(M)):\n",
    "        rgt.seed(rep+1)\n",
    "        #print(rep)\n",
    "        # Split the data into training and test sets (80%-20%)\n",
    "        train_idx = np.random.choice(numm, size=int(0.8 * numm), replace=False)\n",
    "        test_idx = np.setdiff1d(np.arange(numm), train_idx)  # Get the remaining 20% as test set\n",
    "        \n",
    "        X_train = X_COMM_sample.iloc[train_idx]\n",
    "        Y_train = Y_COMM_sample.iloc[train_idx]\n",
    "        X_test = X_COMM_sample.iloc[test_idx]\n",
    "        Y_test = Y_COMM_sample.iloc[test_idx]\n",
    "        \n",
    "        Y_train = pd.to_numeric(Y_train)\n",
    "        Y_test = pd.to_numeric(Y_test)\n",
    "\n",
    "        # benchmark\n",
    "        model_benchmark = Huber(X_train, Y_train, intercept=True)\n",
    "        benchmark = model_benchmark.gd_highdim(lr=0.5, T=T, s=s, tau=None, robust=robust_noiseless, beta0=np.array([]), \n",
    "                                               standardize=False)\n",
    "        beta_huber = benchmark['beta'] \n",
    "        beta_norm = beta_huber.dot(beta_huber)**0.5\n",
    "\n",
    "        # Subsample for initial estimation  \n",
    "        random_rows = np.random.choice(len(X_train), size=n_1, replace=False)\n",
    "        X_subsample = X_train.iloc[random_rows]\n",
    "        Y_subsample = Y_train.iloc[random_rows]\n",
    "        Y_subsample = pd.to_numeric(Y_subsample)\n",
    "        \n",
    "        model_sub = Huber(X_subsample, Y_subsample, intercept=True)\n",
    "        initial = model_sub.gd_highdim(lr=0.5, T=4, s=s, tau=None, robust=robust_noiseless, \n",
    "                                       beta0=np.array([]),  standardize=False) \n",
    "\n",
    "        # Rest for DP Huber  \n",
    "        X_rest = X_train.drop(index=X_train.index[random_rows])\n",
    "        Y_rest = Y_train.drop(index=Y_train.index[random_rows])\n",
    "        beta0 = initial['beta']\n",
    "        \n",
    "        model = Huber(X_rest, Y_rest, intercept=True)\n",
    "        out_Huber_noise_new = model.noisygd_highdim(mu=mu, T=T, delta=delta, lr=lr, beta0=beta0, \n",
    "                                                       s=s, robust_low1=robust_low1,robust_low2=robust_low2, robust_high1=robust, \n",
    "                                                       robust_high2=robust_noise, B_high=B_huber, standardize=False)\n",
    "        \n",
    "\n",
    "         ## Parameters for DP LS \n",
    "        n_ls = int(0.8*numm)\n",
    "        lr_ls = 0.1  ## learning rate for DP LS  \n",
    "        C_ls = 1.01 * beta_norm # feasibility parameter for DP LS \n",
    "        R_ls = 0.1*np.sqrt(2*np.log(n_ls)) \n",
    "        c_x = 1\n",
    "        B_ls = 4*(R_ls+C_ls*c_x)*c_x/np.sqrt(s)\n",
    "        # Center the features for LS\n",
    "        X_cent = X_train.to_numpy() - np.mean(X_train.to_numpy(), axis=0)\n",
    "        Y_cent = Y_train - np.mean(Y_train)\n",
    "        model_ls = Huber(X_cent, Y_cent, intercept=False)\n",
    "        out_LS = model_ls.noisygd_ls(mu=mu, T=T, delta=delta, lr=lr_ls, s=s, R=R_ls, C=C_ls, B=B_ls, \n",
    "                                    beta0=np.array([]), standardize=False)\n",
    "\n",
    "        # MSE calculations\n",
    "        X_test_new = np.concatenate([np.ones((len(X_test), 1)), X_test], axis=1)\n",
    "        Huber_pre = X_test_new @ beta_huber\n",
    "        Noise_Huber_pre = X_test_new @ out_Huber_noise_new['beta2']\n",
    "        \n",
    "        X_test_cent = X_test.to_numpy() - np.mean(X_test.to_numpy(), axis=0)\n",
    "        LS_pre = X_test_cent @ out_LS['beta']\n",
    "\n",
    "        Y_test_cent =  Y_test - np.mean(Y_test)\n",
    "        HD_Huber[rep] = np.mean((Huber_pre - Y_test) ** 2)\n",
    "        HD_Huber_noise[rep] = np.mean((Noise_Huber_pre - Y_test) ** 2)\n",
    "        HD_ls_noise[rep] = np.mean((LS_pre - Y_test_cent) ** 2)\n",
    "\n",
    "    # Average MSE for this sample size\n",
    "    result_Huber.append(np.mean(HD_Huber))\n",
    "    result_Huber_noise.append(np.mean(HD_Huber_noise))\n",
    "    result_ls_noise.append(np.mean(HD_ls_noise))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
